{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e275ab-9a9a-4e04-8b10-466aa021d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"dinov2\")\n",
    "\n",
    "\n",
    "try:\n",
    "    from xformers.ops import cross_entropy\n",
    "\n",
    "    def lossfunc(t, s, temp):\n",
    "        s = s.float()\n",
    "        t = t.float()\n",
    "        if s.ndim == 2:\n",
    "            return -cross_entropy(s.unsqueeze(0), t.unsqueeze(0), temp, bw_inplace=True).squeeze(0)\n",
    "        elif s.ndim == 3:\n",
    "            return -cross_entropy(s, t, temp, bw_inplace=True)\n",
    "\n",
    "except ImportError:\n",
    "\n",
    "    def lossfunc(t, s, temp):\n",
    "        return torch.sum(t * F.log_softmax(s / temp, dim=-1), dim=-1)\n",
    "\n",
    "\n",
    "class iBOTPatchLoss(nn.Module):\n",
    "    def __init__(self, patch_out_dim, student_temp=0.1, center_momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.register_buffer(\"center\", torch.zeros(1, 1, patch_out_dim))\n",
    "        self.updated = True\n",
    "        self.reduce_handle = None\n",
    "        self.len_teacher_patch_tokens = None\n",
    "        self.async_batch_center = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def softmax_center_teacher(self, teacher_patch_tokens, teacher_temp):\n",
    "        self.apply_center_update()\n",
    "        # teacher centering and sharpening\n",
    "        #\n",
    "        # WARNING:\n",
    "        #   as self.center is a float32, everything gets casted to float32 afterwards\n",
    "        #\n",
    "        # teacher_patch_tokens = teacher_patch_tokens.float()\n",
    "        # return F.softmax((teacher_patch_tokens.sub_(self.center.to(teacher_patch_tokens.dtype))).mul_(1 / teacher_temp), dim=-1)\n",
    "\n",
    "        return F.softmax((teacher_patch_tokens - self.center) / teacher_temp, dim=-1)\n",
    "\n",
    "        # this is experimental, keep everything in float16 and let's see what happens:\n",
    "        # return F.softmax((teacher_patch_tokens.sub_(self.center)) / teacher_temp, dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sinkhorn_knopp_teacher(self, teacher_output, teacher_temp, n_masked_patches_tensor, n_iterations=3):\n",
    "        teacher_output = teacher_output.float()\n",
    "        # world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "        Q = torch.exp(teacher_output / teacher_temp).t()  # Q is K-by-B for consistency with notations from our paper\n",
    "        # B = Q.shape[1] * world_size # number of samples to assign\n",
    "        B = n_masked_patches_tensor\n",
    "        dist.all_reduce(B)\n",
    "        K = Q.shape[0]  # how many prototypes\n",
    "\n",
    "        # make the matrix sums to 1\n",
    "        sum_Q = torch.sum(Q)\n",
    "        if dist.is_initialized():\n",
    "            dist.all_reduce(sum_Q)\n",
    "        Q /= sum_Q\n",
    "\n",
    "        for it in range(n_iterations):\n",
    "            # normalize each row: total weight per prototype must be 1/K\n",
    "            sum_of_rows = torch.sum(Q, dim=1, keepdim=True)\n",
    "            if dist.is_initialized():\n",
    "                dist.all_reduce(sum_of_rows)\n",
    "            Q /= sum_of_rows\n",
    "            Q /= K\n",
    "\n",
    "            # normalize each column: total weight per sample must be 1/B\n",
    "            Q /= torch.sum(Q, dim=0, keepdim=True)\n",
    "            Q /= B\n",
    "\n",
    "        Q *= B  # the columns must sum to 1 so that Q is an assignment\n",
    "        return Q.t()\n",
    "\n",
    "    def forward(self, student_patch_tokens, teacher_patch_tokens, student_masks_flat):\n",
    "        \"\"\"\n",
    "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
    "        student_patch_tokens: (B, N, D) tensor\n",
    "        teacher_patch_tokens: (B, N, D) tensor\n",
    "        student_masks_flat: (B, N) tensor\n",
    "        \"\"\"\n",
    "        t = teacher_patch_tokens\n",
    "        s = student_patch_tokens\n",
    "        loss = torch.sum(t * F.log_softmax(s / self.student_temp, dim=-1), dim=-1)\n",
    "        loss = torch.sum(loss * student_masks_flat.float(), dim=-1) / student_masks_flat.sum(dim=-1).clamp(min=1.0)\n",
    "        return -loss.mean()\n",
    "\n",
    "    def forward_masked(\n",
    "        self,\n",
    "        student_patch_tokens_masked,\n",
    "        teacher_patch_tokens_masked,\n",
    "        student_masks_flat,\n",
    "        n_masked_patches=None,\n",
    "        masks_weight=None,\n",
    "    ):\n",
    "        t = teacher_patch_tokens_masked\n",
    "        s = student_patch_tokens_masked\n",
    "        # loss = torch.sum(t * F.log_softmax(s / self.student_temp, dim=-1), dim=-1)\n",
    "        loss = lossfunc(t, s, self.student_temp)\n",
    "        if masks_weight is None:\n",
    "            masks_weight = (\n",
    "                (1 / student_masks_flat.sum(-1).clamp(min=1.0))\n",
    "                .unsqueeze(-1)\n",
    "                .expand_as(student_masks_flat)[student_masks_flat]\n",
    "            )\n",
    "        if n_masked_patches is not None:\n",
    "            loss = loss[:n_masked_patches]\n",
    "        loss = loss * masks_weight\n",
    "        return -loss.sum() / student_masks_flat.shape[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_patch_tokens):\n",
    "        self.reduce_center_update(teacher_patch_tokens)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reduce_center_update(self, teacher_patch_tokens):\n",
    "        self.updated = False\n",
    "        self.len_teacher_patch_tokens = len(teacher_patch_tokens)\n",
    "        self.async_batch_center = torch.sum(teacher_patch_tokens.mean(1), dim=0, keepdim=True)\n",
    "        if dist.is_initialized():\n",
    "            self.reduce_handle = dist.all_reduce(self.async_batch_center, async_op=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_center_update(self):\n",
    "        if self.updated is False:\n",
    "            world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "\n",
    "            if self.reduce_handle is not None:\n",
    "                self.reduce_handle.wait()\n",
    "            _t = self.async_batch_center / (self.len_teacher_patch_tokens * world_size)\n",
    "\n",
    "            self.center = self.center * self.center_momentum + _t * (1 - self.center_momentum)\n",
    "\n",
    "            self.updated = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba2947-80d5-4228-b630-fe3bbc3d8638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
